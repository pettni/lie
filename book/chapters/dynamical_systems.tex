% !TEX root = ../root.tex

\chapter{Dynamical Systems on Lie Groups}

Having defined Lie group derivatives a logical next step is to consider differential equations on Lie groups, i.e. solutions $\x(t)$ to
\begin{equation}
  \label{eq:system}
  \begin{aligned}
    \mathrm{d}^r \x_t = f(t, \x(t)), \\
    \x(0) = \x_0.
  \end{aligned}
\end{equation}
In this chapter we study various properties of this system: its linearization, sensitivity with respect to initial conditions, monotonicity, and finally a method to analyze it via a system on $\mathbb{R}^n$.

For a given initial condition $\x_{0}$ the solution of \eqref{eq:system} at time $t \geq 0$ can be denoted $\phi(t; \x_{0})$ where the \emph{flow} operator $\phi : \mathbb{R} \times \M \rightarrow \M$ is s.t.
\begin{equation}
  \begin{aligned}
    \phi(0; \x_{0})                    & = \x_{0},                            \\
    \mathrm{d}^{r} \phi(t; \x_{0})_{t} & = f\left(t, \phi(t; \x_{0}) \right).
  \end{aligned}
\end{equation}

\begin{remark}
  \label{remark:parameter_initial}
  Parameters and initial conditions are equivalent. A parameter-dependent system
  \begin{equation}
    \begin{aligned}
      \mathrm{d}^r \x_t & = f(t, \x; p_{0}), \\
      \x(t_{0})         & = \x_{0},
    \end{aligned}
  \end{equation}
  is equivalent to the parameter-free system on $\M \times \mathbb{R}^{n}$
  \begin{equation}
    \begin{aligned}
      \mathrm{d}^r (\x, p)_t & =  \left(g(t, \x, p), 0\right), \qquad g(t, \x, p) \coloneq f(t, \x; p), \\
      (\x, p)(t_{0})         & = (\x_0, p_{0}),
    \end{aligned}
  \end{equation}
  where $g(t, \x, p) = f(t, \x; p)$. Conversely, the system
  \begin{equation}
    \begin{aligned}
      \mathrm{d}^{r} \x_{t} & = f(t, \x), \\
      \x(t_{0})             & = \x_{0},
    \end{aligned}
  \end{equation}
  with a non-trivial initial condition is (locally) equivalent to the parameter-dependent system
  \begin{equation}
    \begin{aligned}
      \mathrm{d}^{r} \a_{t} & = g(t, \a; t_{0}, \x_{0}), \qquad g(t, \a; t_{0}, \x_{0}) \coloneq \left[ \mathrm{d}^{r} \exp_{\a} \right]^{-1} \; f(t_{0} + t, \x_{0} \oplus_{r} \a), \\
      \a(0)                 & = 0,
    \end{aligned}
  \end{equation}
  with trivial initial conditions, in the sense that $\Phi^{\x}(t_{0} + t; t_{0}, \x_{0}) = \x_{0} \oplus_{r} \Phi^{\a}(t; t_{0}, \x_{0})$.
\end{remark}


\section{Tangent Space Linearization}

We start by considering the dynamics around a nominal trajectory $(\X_l(t), u_l(t))$. Consider the difference
\begin{equation}
  \a_e = \X(t) \ominus_r \X_l(t)
\end{equation}
between the state of \eqref{eq:system} and the nominal trajectory. Since $\a_e$ takes values in $T_{\X_l(t)} \M \cong \mathbb{R}^n$ the rule of total derivatives in Remark \ref{remark:total_derivative} applies and the time derivative of $\a_e$ is
\begin{equation}
  \label{eq:linearized_system}
  \begin{aligned}
    \frac{\mathrm{d} \a_e}{\mathrm{d}t} = \mathrm{d}^r (\a_e)_t
     & = \mathrm{d}^r (\X \ominus_r \X_l)_\X \mathrm{d}^r \X_t + \mathrm{d}^r (\X \ominus_r \X_l)_{\X_l} \; \mathrm{d}^r (\X_l)_t
    \\
     & \overset{\eqref{eq:d_rminus_fst}, \eqref{eq:d_rminus_snd}}= \left[\mathrm{d}^r \exp_{\a_e} \right]^{-1} f\left(\X_l \oplus_r \a_e, u_l + u_e \right) - \left[\mathrm{d}^l \exp _{\a_e} \right]^{-1} \; \mathrm{d}^r (\X_l)_t,
  \end{aligned}
\end{equation}
Setting $(\a_e, u_e) = (0, 0)$ yields the linear time-varying system
\begin{equation}
  \frac{\mathrm{d}}{\mathrm{d}t} \a_e = A(t) \a_e + B(t) u_e + E(t),
\end{equation}
where, since $\mathrm{d}^r \exp_0 = \mathrm{d}^l \exp_0 = I$,
\begin{align}
  A(t) & \coloneq \left. \frac{\mathrm{d}}{\mathrm{d}\a_e} \right|_{\a_e = 0} \left[\mathrm{d}^r \exp_{\a_e} \right]^{-1} f\left(\X_l(t) \oplus_r \a_e, u_l(t)  \right), \\
  B(t) & \coloneq \left. \frac{\mathrm{d}}{\mathrm{d}u_e} \right|_{u_e = 0} f\left(\X_l(t), u_l(t) + u_e \right),                                                        \\
  E(t) & \coloneq f\left(\X_l(t), u_l(t) \right) - \mathrm{d}^r(\X_l)_t.
\end{align}

\todo[inline]{May be able to simplify derivative of dexpinv further}

\section{Sensitivity Analysis}

Next we study the sensitivity of $\phi(t; \x_0)$ with respect to the initial conditon. Due to the equivalence between parameters and initial conditions as discussed in Remark \ref{remark:parameter_initial}, sensitivity with respect to the initial conditons can also be used to figure the sensitivity with respect to paramters.

Global derivative on matrix form $\Phi = \hat \phi$.
\begin{equation}
  \dot \Phi(t; \x_{0}) = \Phi(t; \x_{0}) \left( \mathrm{d}^{r} \Phi(t; \x_{0})_{t} \right )^{\wedge} = \Phi(t; \x_{0})  \hat f\left(t, \Phi(t; \x_{0}) \right).
\end{equation}
Derivative of inverse
\begin{equation}
  \begin{aligned}
    0 = \frac{\mathrm{d}}{\mathrm{d}t} \Phi(t; \x_{0}) \circ \Phi(t; \x_{0})^{-1} = \dot \Phi(t; \x_{0}) \Phi(t; \x_{0})^{-1} + \Phi(t; \x_{0}) \frac{\mathrm{d}}{\mathrm{d}t} \Phi(t; \x_{0})^{-1} \\
    \implies \; \; \frac{\mathrm{d}}{\mathrm{d}t} \phi(t; \x_{0})^{-1} = \Phi(t; \x_{0})^{-1} \dot \Phi(t; \x_{0}) \Phi(t; \x_{0})^{-1}.
  \end{aligned}
\end{equation}
We can then evaluate how $\mathrm{d}^{r} \Phi(t; \x_{0})_{t}$ depends on $t$ by moving to global derivatives and changing the order of integration.
\begin{equation*}
  \begin{aligned}
    \frac{\mathrm{d}}{\mathrm{d}t}
     & \left( \mathrm{d}^r \Phi(t; \x_0)_{\x_0} \a \right)
    = \frac{\mathrm{d}}{\mathrm{d}t} \left( \Phi(t; \x_{0})^{-1} \left. \frac{\mathrm{d}}{\mathrm{d} \tau} \right|_{\tau = 0} \Phi(t; \x_{0} \oplus \tau \a) \right)^{\vee}                                                                                                                          \\
     & = \left( \left( -\Phi(t; \x_{0})^{-1} \dot \Phi(t; \x_{0}) \Phi(t; \x_{0}) \right)^{-1} \frac{\mathrm{d}}{\mathrm{d}\tau} \Phi(t; \x_{0} \oplus \tau \a) +  \Phi(t; \x_{0})^{-1} \left. \frac{\mathrm{d}}{\mathrm{d}\tau} \right|_{\tau=0} \dot \Phi(t; \x_{0} \oplus \tau \a) \right)^{\vee} \\
     & = \left( -\hat f\left(t; \Phi(t; \x_{0})\right) \Phi(t; \x_{0})^{-1} \left. \frac{\mathrm{d}}{\mathrm{d}\tau} \right|_{\tau = 0} \Phi(t; \x_{0} \oplus \tau \a) \right)^\vee                                                                                                                  \\
     & + \left( \Phi(t; \x_{0})^{-1} \left. \frac{\mathrm{d}}{\mathrm{d}\tau} \right|_{\tau = 0} \Phi(t; \x_{0} \oplus \tau \a) \hat f(t; \Phi(t; \x_{0} \oplus \tau \a)) \right)^{\vee}                                                                                                             \\
     & = - \left( \hat f(t; \Phi(t; \x_{0})) \left( \mathrm{d}^{r} \Phi(t; \x_{0})_{\x_{0}} \a \right)^{\wedge}
    + \left( \mathrm{d}^{r} \Phi(t; \x_{0})_{\x_{0}} \a \right)^{\wedge} \hat f(t; \Phi(t; \x_{0}))  + \left. \frac{\mathrm{d}}{\mathrm{d}\tau} \right|_{\tau = 0} \hat f(t; \Phi(t; \x_{0} \oplus \tau \a)) \right)^{\vee}                                                                          \\
     & = -\left[  f(t; \Phi(t; \x_{0})), \mathrm{d}^{r} \Phi(t; \x_{0})_{\x_{0}} \a\right] + \mathrm{d}^{r} f(t, \Phi(t; \x_{0}))_{\x_{0}} \a                                                                                                                                                        \\
     & = -\ad_{f(t; \Phi(t; \x_{0}))} \mathrm{d}^{r} \Phi(t; \x_{0})_{\x_{0}} \a + \mathrm{d}^{r} f(t, \x)_{\x = \Phi(t; \x_{0})} \mathrm{d}^{r} \Phi(t; \x_{0}))_{\x_{0}} \a.
  \end{aligned}
\end{equation*}
\begin{important}
  The sensitivity $S(t) \coloneq \mathrm{d}^r \Phi(t; \x_{0})_{\x_{0}}$ satisfies the matrix-valued ODE
  \begin{equation}
    \label{eq:sensitivity_direct}
    \begin{aligned}
      \frac{\mathrm{d}}{\mathrm{d}t} S(t) & = \left(-\ad_{f(t, \Phi(t; \x_{0}))} + \left. \mathrm{d}^{r} f_{\x} \right|_{\x = \Phi(t; \x_{0})} \right) S(t), \\
      S(0)                                & = I.
    \end{aligned}
  \end{equation}
\end{important}

\subsection{Example}

\begin{example}
  If $\mathrm{d}^{r} \x_{t} = f(\x) \equiv \a$, then $\x(t) = \x_{0} \exp(t \a)$ and we get
  \begin{equation}
    \mathrm{d}^{r} (\x(t))_{\x_{0}} \overset{\eqref{eq:d_composition_rght_fst}} = \bAd_{\exp(-t \a)}.
  \end{equation}

  We furthermore know from Lemma \ref{lem:d_bad_exp} that
  \begin{equation}
    \label{eq:1}
    \frac{\mathrm{d}}{\mathrm{d}t} \bAd_{\exp(-t \a)} = -\ad_{\a} \bAd_{\exp(-t \a)},
  \end{equation}
  i.e. the sensitivity equations are
  \begin{equation}
    \label{eq:13}
    \frac{\mathrm{d}}{\mathrm{d}t} S(t) = -\ad_{\a} S(t),
  \end{equation}
  which was expected from \eqref{eq:sensitivity_direct} since $f$ is constant.
\end{example}

\section{Monotonicity}

\todo[inline]{Very much a work in progress}

Monotonicity is a useful property of dynamical systems that can be leveraged in order to bound the envelope of possible behaviors by a small number of extremal trajectories. For instance, a forward-traveling vehicle that is trying to stop is always better of the less it accelerates, which means that it is sufficient to analyze it's minimal acceleration in order to determine whether it can stop in time.

\paragraph{Monotonicity on $\mathbb{R}^n$} Monotonicity is usually defined with respect to a \emph{cone}---a set with the property that $0 \in K$ and $\x \in K \implies \alpha K \in K$ for $\alpha \geq 0$. For a cone we can define an ordering $\preceq_K$ such that
\begin{equation}
  x \preceq_K y \quad \Longleftrightarrow \quad y - x \in K.
\end{equation}

Monotonicity of a function $f$ can then be defined as the following property:
\begin{equation}
  x \preceq_K y  \quad \implies \quad f(x) \preceq_K f(y).
\end{equation}

\paragraph{Monotonicity on Lie groups}
The usual notion of monotonicity only applies for \emph{ordered spaces}, which is a property that is not present in the usual Lie groups used in robotics. Indeed, for a circle ordering makes little sense.  However, the tangent space of a Lie group is monotone which makes it possible to define a notion of \emph{local monotonicity} in a way that is analogous to the Euclidean case.

\begin{definition}
  \label{def:monotonicity}
  A function $f : \M \rightarrow \N$ is locally monotone around $\Z \in \M$ with respect to a cone $K \subset \mathfrak m \cong \mathbb{R}^n$ if for all $\a, \b$ that are sufficiently small it holds that
  \begin{equation}
    \a \preceq_K \b \quad \implies \quad f(\Z \oplus_r \a) \ominus_r f(\Z) \preceq_K f(\Z \oplus_r \b) \ominus f(\Z).
  \end{equation}
\end{definition}
When $\M$ and $\N$ are Euclidean spaces $\Z$ can be set to zero to retrieve the original definition.

\begin{itemize}
  \item Define mixed monotonicity corresponding to Def \eqref{def:monotonicity}.
  \item Derive a jacobian condition on $f$ for mixed monotonicity that is analogous to sign-stability?
  \item Create a dynamical system that over-approximates reach sets of one of these forms:
        \begin{itemize}
          \item MID-DOWN-UP: $\Alpha(\X, l, u) = \{ \Y : l \preceq_K \Y \ominus_r \X \preceq_K u \}$
          \item MID-SINGLE: $\Alpha(\X, k) = \{ \Y : -k \preceq_K \Y \ominus_r \X \preceq_K k \}$
          \item MID-RADIUS: $\Alpha(\X, r) = \{ \Y : \| \Y \ominus_r \X \| < r \}$
          \item The values $l, u$ need to be twisted as part of the mapping
        \end{itemize}
\end{itemize}

The derivative of the mapping $\a \mapsto f(\Z \oplus_r \a) \ominus f(\Z)$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is
\begin{equation}
  \mathrm{d}^r \left( f(\Z \oplus_r \a) \ominus f(\Z) \right)_\a = \left[ \mathrm{d}^r \exp_{f(\Z \oplus_r \a) \ominus f(\Z)} \right]^{-1} \; \mathrm{d}^r f_{\Z \oplus \a} \; \mathrm{d}^r \exp_\a
\end{equation}
It follows that this is what we need to make sign-stable, so the decomposition should depend on it. Challenge is that the decomposition may have to depend on both $\a$ and on $\Z$.

\paragraph{Reach mapping:}

Set $\left\{ \X : \underline \a \preceq_K \X \ominus_r \Z \preceq_K \overline \a \right\}$

Decomposition function $g$ s.t. $f(\Z \oplus \a) \ominus f(\Z) = g_{\Z}(\a, \a)$

Mapped set:
\begin{equation}
  \left\{ \X : g_\Z(\underline \a, \overline \a) \preceq_K \X \ominus f(\Z) \preceq_K g_\Z(\overline \a, \underline \a) \right\}.
\end{equation}

\begin{itemize}
  \item How to go from monotonicity of $f : \M \rightarrow \mathbb{R}^m$ to monotonicity of the flow $\phi : \M \rightarrow \M$?
  \item How is decomposition function done in practice? Like in Necmiyes paper?
\end{itemize}

\section{The Magnus Expansion}

For the case when the right-hand side in \eqref{eq:system} only depends on $t$,
\begin{equation}
  \label{eq:system_time}
  \mathrm{d}^r \x_t = \a(t),
\end{equation}
we can posit that the solution be on the form
\begin{equation}
  \x(t) = \exp (\Omega(t)), \quad \Omega(t) \in \M.
\end{equation}
From the differentiation rules it follows that
\begin{equation}
  \a(t) = \mathrm{d}^r \x_t = \mathrm{d}^r \exp_{\Omega(t)} \frac{\mathrm{d}}{\mathrm{d}t}\Omega(t),
\end{equation}
which yields an ODE for $\Omega(t)$.
\begin{important}
  \begin{theorem}
    \label{thm:magnus_solution}
    The solution of the time-varying ODE \eqref{eq:system_time} is given by
    \begin{equation}
      \x(t) = \exp(\Omega(t)) \x_0,
    \end{equation}
    where $\Omega(t)$ satisfies the initial-value problem
    \begin{equation}
      \label{eq:magnus_omega_ode}
      \begin{aligned}
        \frac{\mathrm{d}}{\mathrm{d}t} \Omega(t) &= \left(\mathrm{d}^r \exp_{\Omega(t)}\right)^{-1} \a(t), \\
        \Omega(0) &= 0.
      \end{aligned}
    \end{equation}
  \end{theorem}
\end{important}

The initial value problem for $\Omega$ may still be challenging to solve in case an expression for $\left(\mathrm{d}^r \exp_\a \right)^{-1}$ is not available. The \textbf{Magnus expansion} is obtained by setting $\a = \epsilon \tilde \a$ and expressing $\Omega$ as a series
\begin{equation}
  \Omega(t) = \sum_{k \geq 1} \epsilon^k \Omega_k(t).
\end{equation}
Inserting this in \eqref{eq:magnus_omega_ode} and comparing powers of $\epsilon$ yields
\begin{equation}
  \begin{aligned}
    \Omega_1(t) & = \int_0^t \a(s_1) \mathrm{d}s_1,                                                                                                                                                                                 \\
    \Omega_2(t) & = -\frac{1}{2} \int_0^t \left[ \Omega_1(s_1), \a(s_1) \right] \mathrm{d}s_1 = \frac{1}{2} \int_0^t \int_{0}^{s_2} \left[ \a(s_1), \a(s_2) \right] \mathrm{d} s_2 \mathrm{d} s_1,                                    \\
    \Omega_3(t) & = \frac{1}{6} \int_{0}^t \int_{0}^{s_1} \int_{0}^{s_2} \left[ \a(s_1), \left[ \a(s_2), \a(s_3) \right] \right] + \left[ \left[ \a(s_1), \a(s_2) \right], \a(s_3) \right] \mathrm{d} s_3 \mathrm{d} s_2 \mathrm{d} s_1,
  \end{aligned}
\end{equation}
and so on for higher powers of $k$.

\subsection{Example}

Consider the initial value problem on $\mathbb{SE}(2)$:
\begin{equation}
  \dot \X(t) = \A(t) \X(t), \quad \X(0) = \X_0, \quad \X \in \mathbb{SE}(2), \quad \A \in \mathfrak{se}(2).
\end{equation}
We assume that $\A(t) = \hat \a(t)$ is a known curve, i.e.
\begin{equation}
  \label{eq:se2matrix}
  \A(t) = \begin{bmatrix}
    0 & -\theta(t) & u(t) \\ \theta(t) & 0 & v(t) \\ 0 & 0 & 0
  \end{bmatrix}
\end{equation}
According to Theorem \ref{thm:magnus_solution} the solution is then
\begin{equation}
  \x(t) = \Exp(\Omega(t)) \x_0 = \Exp \left( \sum_{k \geq 0} \Omega_k(t) \right) \x_0.
\end{equation}
The Lie algebra $\mathfrak{se}(2)$ is not nilpotent, so the exact solution requires the full Magnus expansion. Below we develop an approximate solution corresponding to the first two terms
\begin{equation}
  \x(t) \approx \Exp \left( \int_0^t A(t) \mathrm{d} t + \frac{1}{2} \int_{0}^t \left(\int_{0}^{t_1} \left[ A(t_1), A(t_2) \right] \mathrm{d} t_2 \right) \mathrm{d} t_1 \right) \x_0.
\end{equation}
To find $\Omega_2$ we consider the commutator of matrices in $\mathfrak{se}(2)$ on the form \eqref{eq:se2matrix}:
\begin{equation*}
  \begin{aligned}
    \left[ A(t_1), A(t_2) \right] & = \begin{bmatrix}
      0 & -\theta(t_1) & u(t_1) \\ \theta(t_1) & 0 & v(t_1) \\ 0 & 0 & 0
    \end{bmatrix} \begin{bmatrix}
      0 & -\theta(t_2) & u(t_2) \\ \theta(t_2) & 0 & v(t_2) \\ 0 & 0 & 0
    \end{bmatrix}   \\
                                  & - \begin{bmatrix}
      0 & -\theta(t_2) & u(t_2) \\ \theta(t_2) & 0 & v(t_2) \\ 0 & 0 & 0
    \end{bmatrix} \begin{bmatrix}
      0 & -\theta(t_1) & u(t_1) \\ \theta(t_1) & 0 & v(t_1) \\ 0 & 0 & 0
    \end{bmatrix}   \\
                                  & = \begin{bmatrix}
      -\theta(t_1) \theta(t_2) & 0                        & -\theta(t_1)v(t_2) \\
      0                        & -\theta(t_1) \theta(t_2) & \theta(t_1)u(t_2)  \\
      0                        & 0                        & 0
    \end{bmatrix} - \begin{bmatrix}
      -\theta(t_1) \theta(t_2) & 0                        & -\theta(t_2) v(t_1) \\
      0                        & -\theta(t_1) \theta(t_2) & \theta(t_2) u(t_1)  \\
      0                        & 0                        & 0
    \end{bmatrix} \\
                                  & = \begin{bmatrix}
      0 & 0 & -\theta(t_1) v(t_2) + \theta(t_2) v(t_1) \\
      0 & 0 & \theta(t_1) u(t_2) - \theta(t_2) u(t_1)  \\
      0 & 0 & 0
    \end{bmatrix}.
  \end{aligned}
\end{equation*}

\begin{important}
  In the affine case where $\theta(t) = \theta_0 + a_\theta t$, and similarly for $u$ and $v$, we get after evaluating the integrals:
  \begin{equation*}
    \begin{bmatrix}
      a(t) \\
      b(t) \\
      x(t) \\
      y(t)
    \end{bmatrix} \approx \exp \left( \begin{bmatrix} \theta_0 t + a_\theta t \frac{t^2}{2}                                                 \\
      u_0 t + a_u \frac{t^2}{2} + \theta_0 a_v \frac{t^3}{12} - a_\theta v_0 \frac{t^3}{12} \\
      v_0 t + a_v \frac{t^2}{2} - \theta_0 a_u \frac{t^3}{12} + a_\theta u_0 \frac{t^3}{12}\end{bmatrix} \right).
  \end{equation*}
\end{important}
