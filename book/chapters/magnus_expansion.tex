% !TEX root = ../manuscript.tex

\chapter{The Magnus Expansion}

\todo[inline]{Figure out what happens in the body-velocity ODE $\dot \x = \x \lx$}

\todo[inline]{Remove things that are duplicated in Derivatives chapter}

\section{Preliminaries}

The Bernoulli numbers are defined by
\begin{equation}
  \label{eq:bernoulli_number_definition}
  \frac{t}{e^t - 1} = \sum_{n=0}^\infty \frac{B_n}{n!} t^n.
\end{equation}

\section{The Lie Group Adjoint}

We can consider ``higher order'' Lie brackets and introduce an adjoint operator on the Lie Algebra to simplify notation.  In particular, define:
\begin{equation}
  \begin{aligned}
    \ad_\lx^0 \ly & : = \lx                                                                                                              \\
    \ad_\lx^1 \ly & : = \ad_\lx \ly = [\lx,\ly]                                                                                          \\
    \ad_\lx^2 \ly & : =[\lx,  \ad_\lx \ly] = \underbrace{[\lx,[\lx,\ly]]}_{2-\mathrm{times}}                                             \\
                  & \vdots                                                                                                               \\
    \ad_\lx^k \ly & := [\lx, \ad_\lx^{k - 1} \ly] = \underbrace{[\lx, [ \lx , \ldots, [\lx,\ly]]]}_{k-\mathrm{times}} , \qquad k \geq 1.
  \end{aligned}
\end{equation}
Note that this adjoint operator is different from the Lie group utilized above.

Define also the exponential of the adjoint as the formal expansion
\begin{equation}
  \label{eq:adjoint_exponential}
  \Exp(\ad_\lx) \coloneq \sum_{k \geq 0} \frac{\ad_\lx^k}{k!}.
\end{equation}

\begin{lemma}
  \label{lemma:expadj}
  \begin{equation}
    \label{eq:adjoint_exponential_formula}
    \Exp(\ad_\lx) \; \ly = \Exp(\lx) \; \ly \; \Exp(-\lx).
  \end{equation}
\end{lemma}
\begin{proof}
  By expanding the right-hand side in \eqref{eq:adjoint_exponential_formula} we obtain
  \begin{equation}
    \label{eq:magnus:expansion}
    \sum_{k \geq 0} \sum_{i = 0}^{k} \frac{\lx^i \ly (-\lx)^{k-i}}{i! (k - i)!}.
  \end{equation}
  We next show by induction that the summands in \eqref{eq:adjoint_exponential} and \eqref{eq:magnus:expansion} are equal for each value of $k$. Equality evidently holds for the base case $k=0$. Assume that it holds for $k-1$, i.e. that
  \begin{equation}
    \frac{\ad_\lx^{k-1} \ly}{(k-1)!} = \sum_{i=0}^{k-1} \frac{\lx^i B (-\lx)^{k-1-i}}{i! (k-1-i)!}.
  \end{equation}
  Then we have that
  \begin{equation*}
    \begin{aligned}
      \frac{\ad_\lx^{k} \ly}{k!} = \frac{1}{k} \left[ \lx \frac{(\ad_\lx^{k-1} \ly)}{(k-1)!} - \frac{(\ad_\lx^{k-1} \ly)}{(k-1)!} \lx \right] =
      \frac{1}{k} \left[ \sum_{i=0}^{k-1} \frac{\lx^{i+1} B (-\lx)^{k-1-i}}{i! (k-1-i)!} + \sum_{i=0}^{k-1} \frac{\lx^i B (-\lx)^{k-i}}{i! (k-1-i)!} \right] \\
      = \frac{1}{k} \left[ \sum_{i=0}^{k-1} \frac{\lx^i B (-\lx)^{k-i}}{i! (k-1-i)!} +  \sum_{i=1}^{k} \frac{\lx^{i} B (-\lx)^{k-i}}{(i-1)! (k-i)!} \right] = \frac{\ly (-\lx)^k}{k!} + \sum_{i=1}^{k-1} c_i A^i B (-A)^{k-i} + \frac{\lx^k B}{k!},
    \end{aligned}
  \end{equation*}
  where $c_i = \frac{1}{k} \left( \frac{1}{i!(k-1-i)!} + \frac{1}{(i-1)!(k-i)!} \right)$ and it can be verified that $c_i = \frac{1}{i!(k-i)!}$ as required.
\end{proof}

We are interested in solutions of the system
\begin{equation}
  \label{eq:magnus_ode}
  \frac{\mathrm{d}}{\mathrm{d}t} \x(t) = \lx(t) \x(t), \quad \x(t) \in \X, A(t) \in \lX.
\end{equation}
If $A$ is constant in time the solution is given by the exponential, but in the general case the solution is more involved. The integrating factor technique used in scalar ODEs does not work since $A(t)$ and $\dot A(t)$ do not necessarily commute.

The Magnus Expansion approach is to posit that the solution of \eqref{eq:magnus_ode} is of the form
\begin{equation}
  \x(t) = \Exp (\Omega(t)), \quad \Omega(t) \in \lX.
\end{equation}

Consider $\x(t, \sigma) = \frac{\partial }{\partial t} \left[ \Exp(\sigma \Omega(t)) \right] \Exp(-\sigma \Omega(t))$. Differentiating with respect to $\sigma$ yields
\begin{equation}
  \begin{aligned}
    \frac{\partial}{\partial \sigma} \x(t, \sigma) = \frac{\partial}{\partial t} \left[ \Exp(\sigma \Omega) \Omega \right] \Exp(-\sigma \Omega) - \frac{\partial}{\partial t} \left[ \Exp(\sigma \Omega) \right] \Omega \Exp(-\sigma \Omega) =  \Exp(\sigma \Omega) \frac{\mathrm{d} \Omega}{\mathrm{d}t} \Exp(-\sigma \Omega) \\
    = \Exp (\ad_{\sigma \Omega}) \frac{\mathrm{d} \Omega}{\mathrm{d}t} = \sum_{k \geq 0} \frac{\sigma^k}{k!} \ad_{\Omega}^k \frac{\mathrm{d} \Omega}{\mathrm{d}t}.
  \end{aligned}
\end{equation}
We can therefore write
\begin{equation}
  \x(t, 1) = \int_0^1 \frac{\partial}{\partial \sigma} \x(t, \sigma) \mathrm{d}\sigma = \sum_{k \geq 0} \frac{\ad_{\Omega}^k}{(k+1)!}  \frac{\mathrm{d} \Omega}{\mathrm{d}t}
\end{equation}
and we have the following (where the second formulation follows by multiplying from the left with $\Exp(\Omega(t))$ and utilizing Lemma \ref{lemma:expadj}):
\begin{equation}
  \label{eq:magnus_matrix_ode}
  \frac{\mathrm{d}}{\mathrm{d}t} \Exp(\Omega(t)) = \left(\mathrm{d} \Exp_{\Omega(t)} \frac{\mathrm{d} \Omega}{\mathrm{d}t} \right) \Exp(\Omega(t)) = \Exp(\Omega(t)) \left( \mathrm{d} \Exp_{-\Omega(t)} \frac{\mathrm{d} \Omega}{\mathrm{d}t} \right),
\end{equation}
where, formally, $\mathrm{d} \Exp_\Omega : T \lX_\Omega \rightarrow \lX$ is the derivative of the exponential map $\Exp: \lX \rightarrow \X$ \textcolor{red}{around zero?}
\begin{equation}
  \mathrm{d} \Exp_\Omega = \sum_{k \geq 0} \frac{\ad_{\Omega}^k}{(k+1)!} = \frac{1}{\ad_\Omega} \sum_{k \geq 0} \frac{\ad_{\Omega}^{k+1}}{(k+1)!} = \frac{\Exp(\ad_\Omega) - I}{\ad_\Omega}.
\end{equation}

Under certain conditions the linear operator $\mathrm{d} \Exp_\Omega$ can be inverted, and from the definition of the Bernoulli numbers in \eqref{eq:bernoulli_number_definition} we obtain
\begin{equation}
  \mathrm{d} \Exp^{-1}_\Omega \ly = \frac{\ad_\Omega}{\Exp(\ad_\Omega) - I} \ly = \sum_{k \geq 0} \frac{B_k}{k!} \ad_\Omega^k \ly.
\end{equation}

\begin{important}
  \begin{theorem}
    \label{thm:magnus_solution}
    The solution of the time-varying ODE
    \begin{equation}
      \frac{\mathrm{d}}{\mathrm{d}t} \x(t) = \lx(t) \x(t), \quad \x(0) = \x_0,
    \end{equation}
    is given by
    \begin{equation}
      \x(t) = \Exp(\Omega(t)) \x_0,
    \end{equation}
    where $\Omega(t)$ satisfies the initial-value problem
    \begin{equation}
      \label{eq:magnus_omega_ode}
      \Omega(0) = 0, \quad \frac{\mathrm{d}}{\mathrm{d}t} \Omega(t) = \mathrm{d}\Exp^{-1}_{\Omega(t)} \lx(t) \coloneq \sum_{k \geq 0} \frac{B_k}{k!} \ad_{\Omega(t)}^k A(t).
    \end{equation}
  \end{theorem}
\end{important}
\begin{proof}
  Consider $\y(t) = \Exp (\Omega(t)) \x_0$, by \eqref{eq:magnus_matrix_ode} it satisfies
  \begin{equation}
    \frac{\mathrm{d}}{\mathrm{d}t} \y(t) = \mathrm{d} \Exp_{\Omega(t)}(\Omega'(t)) \Exp(\Omega(t)) \x_0 = \mathrm{d} \Exp_{\Omega(t)}(\Omega'(t)) \y(t),
  \end{equation}
  and we see that $A(t) = \mathrm{d} \Exp_{\Omega(t)}(\Omega'(t))$. Applying the inverse operator results in the theorem statement.
\end{proof}

\begin{remark}
  For a body-velocity problem
  \begin{equation}
    \dot \x = \x \lx, \quad \x(0) = \x_0,
  \end{equation}
  the second expression in \eqref{eq:magnus_matrix_ode} can be used to show that the solution is
  \begin{equation}
    \x(t) = \x_0 \Exp (\Omega(t)),
  \end{equation}
  where $\Omega$ satisfies
  \begin{equation}
    \frac{\mathrm{d}}{\mathrm{d}t}\Omega(t) = \mathrm{d} \Exp^{-1}_{-\Omega(t)} A(t) = \sum_{k \geq 0} \frac{B_k}{k!} \ad_{-\Omega(t)}^k A(t), \quad \Omega(0) = 0.
  \end{equation}
\end{remark}

\todo[inline]{Should verify this with a nilpotent Lie Algebra: The Lie group of invertible upper triangular matrices is the algebra of upper triangular invertible matrices!}

The initial value problem for $\Omega$ is still challenging to solve. The \textbf{Magnus expansion} is obtained by setting $A = \epsilon \tilde A$ and expressing $\Omega$ as a series
\begin{equation}
  \Omega(t) = \sum_{k \geq 1} \epsilon^k \Omega_k(t).
\end{equation}
Inserting this in \eqref{eq:magnus_omega_ode} and comparing powers of $\epsilon$ yields
\begin{equation}
  \begin{aligned}
    \Omega_1(t) & = \int_0^t A(s_1) \mathrm{d}s_1,                                                                                                                                                                                 \\
    \Omega_2(t) & = -\frac{1}{2} \int_0^t \left[ \Omega_1(s_1), A(s_1) \right] \mathrm{d}s_1 = \frac{1}{2} \int_0^t \int_{0}^{s_2} \left[ A(s_1), A(s_2) \right] \mathrm{d} s_2 \mathrm{d} s_1,                                    \\
    \Omega_3(t) & = \frac{1}{6} \int_{0}^t \int_{0}^{s_1} \int_{0}^{s_2} \left[ A(s_1), \left[ A(s_2), A(s_3) \right] \right] + \left[ \left[ A(s_1), A(s_2) \right], A(s_3) \right] \mathrm{d} s_3 \mathrm{d} s_2 \mathrm{d} s_1.
  \end{aligned}
\end{equation}


\section{Example}

Consider the initial value problem on $\mathbb{SE}(2)$:
\begin{equation}
  \dot \x(t) = \lx(t) \x(t), \quad \x(0) = \x_0, \quad \x \in \mathbb{SE}(2), \quad \lx \in \mathfrak{se}(2).
\end{equation}
We assume that $\lx(t) = \hat \lxp(t)$ is a known curve, i.e.
\begin{equation}
  \label{eq:se2matrix}
  \lx(t) = \begin{bmatrix}
    0 & -\theta(t) & u(t) \\ \theta(t) & 0 & v(t) \\ 0 & 0 & 0
  \end{bmatrix}
\end{equation}
According to Theorem \ref{thm:magnus_solution} the solution is then
\begin{equation}
  \x(t) = \Exp(\Omega(t)) \x_0 = \Exp \left( \sum_{k \geq 0} \Omega_k(t) \right) \x_0.
\end{equation}
The Lie algebra $\mathfrak{se}(2)$ is not nilpotent, so the exact solution requires the full Magnus expansion. Below we develop an approximate solution corresponding to the first two terms
\begin{equation}
  \x(t) \approx \Exp \left( \int_0^t A(t) \mathrm{d} t + \frac{1}{2} \int_{0}^t \left(\int_{0}^{t_1} \left[ A(t_1), A(t_2) \right] \mathrm{d} t_2 \right) \mathrm{d} t_1 \right) \x_0.
\end{equation}
To find $\Omega_2$ we consider the commutator of matrices in $\mathfrak{se}(2)$ on the form \eqref{eq:se2matrix}:
\begin{equation*}
  \begin{aligned}
    \left[ A(t_1), A(t_2) \right] & = \begin{bmatrix}
      0 & -\theta(t_1) & u(t_1) \\ \theta(t_1) & 0 & v(t_1) \\ 0 & 0 & 0
    \end{bmatrix} \begin{bmatrix}
      0 & -\theta(t_2) & u(t_2) \\ \theta(t_2) & 0 & v(t_2) \\ 0 & 0 & 0
    \end{bmatrix} - \begin{bmatrix}
      0 & -\theta(t_2) & u(t_2) \\ \theta(t_2) & 0 & v(t_2) \\ 0 & 0 & 0
    \end{bmatrix} \begin{bmatrix}
      0 & -\theta(t_1) & u(t_1) \\ \theta(t_1) & 0 & v(t_1) \\ 0 & 0 & 0
    \end{bmatrix} \\
                                  & = \begin{bmatrix}
      -\theta(t_1) \theta(t_2) & 0                        & -\theta(t_1)v(t_2) \\
      0                        & -\theta(t_1) \theta(t_2) & \theta(t_1)u(t_2)  \\
      0                        & 0                        & 0
    \end{bmatrix} - \begin{bmatrix}
      -\theta(t_1) \theta(t_2) & 0                        & -\theta(t_2) v(t_1) \\
      0                        & -\theta(t_1) \theta(t_2) & \theta(t_2) u(t_1)  \\
      0                        & 0                        & 0
    \end{bmatrix}                                                       \\
                                  & = \begin{bmatrix}
      0 & 0 & -\theta(t_1) v(t_2) + \theta(t_2) v(t_1) \\
      0 & 0 & \theta(t_1) u(t_2) - \theta(t_2) u(t_1)  \\
      0 & 0 & 0
    \end{bmatrix}.
  \end{aligned}
\end{equation*}

\begin{important}
  In the affine case where $\theta(t) = \theta_0 + a_\theta t$, and similarly for $u$ and $v$, we get after evaluating the integrals:
  \begin{equation*}
    \begin{bmatrix}
      a(t) \\
      b(t) \\
      x(t) \\
      y(t)
    \end{bmatrix} \approx \exp \left( \begin{bmatrix} \theta_0 t + a_\theta t \frac{t^2}{2}                                                 \\
      u_0 t + a_u \frac{t^2}{2} + \theta_0 a_v \frac{t^3}{12} - a_\theta v_0 \frac{t^3}{12} \\
      v_0 t + a_v \frac{t^2}{2} - \theta_0 a_u \frac{t^3}{12} + a_\theta u_0 \frac{t^3}{12}\end{bmatrix} \right).
  \end{equation*}
\end{important}