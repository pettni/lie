% !TEX root = ../root.tex

\chapter{Nonlinear Least Squares}

Like how Lie groups thread the line between linear and nonlinear manifolds, the same can be said for the role of nonlinear least squares in optimization, which is a type of optimization problem is rich enough for to model a wide variety of situations, yet structured enough to be amenable to practical algorithms.

A non-linear least squares problem has the general form
\begin{equation}
  \label{eq:nlsq}
  \min_{\x \in \M} \frac{1}{2} \sum_{i=1}^{N} \left\| r_{i}(\x) \right\|^{2}, \quad r_{i} : \mathcal M \rightarrow \mathbb{R}^{n_{r_{i}}}.
\end{equation}
The manifold $\M$ can be a Lie group or a Lie group product $\x = (\x_{1}, \ldots, \x_{k}) \in \M_{1} \times \ldots \times \M_{k}$. For the latter case, typically not every residual depends on each member of the bundle, i.e. $r_{i}(\x) = r_{i}\left( \{ \x_{j}\}_{j \in I_{i}} \right)$ where $I_{i} \subset \{ 1, \ldots, k\}$ is a subset of variables.

\begin{remark}
  An equivalent problem with a single residual is $\min_{\x \in \M} \frac{1}{2} \| r(\x) \|^{2}$ for
  \begin{equation}
    r(\x) =
    \begin{bmatrix}
      r_{1}(\x) \\
      \vdots    \\
      r_{k}(\x)
    \end{bmatrix}.
  \end{equation}
  Although the single residual formulation simplifies notation somewhat, in practice it is for large problems important to leverage the sparsity structure which is better exposed in \eqref{eq:nlsq}.
\end{remark}


\section{Solution Sensitivity}

In many applications the residuals $r_{i}(\x)$ are obtained from data and are therefore associated with uncertainty. In this situation it is natural to ask how sensitive the optimal solution of the nonlinear least squares problem is to noise in the data. Assume that the noise associated with each residual is Gaussian and independent of other residuals, i.e. that
\begin{equation}
  r_{i}(\x) \sim \mathcal N\left(\bar r_{i}(\x), I \right),
\end{equation}
and consider a point $\bar \x$. We expand the objective using a Taylor approximation as
\begin{equation}
  \min_{\bar \x \in \M} \frac{1}{2} \sum_{i=1}^{N} \left\| r_{i}(\bar \x) \right\|^{2} \approx \min_{\a \in T_{\bar \x} \M} \frac{1}{2}  \sum_{i=1}^{N} \left\| r_{i}(\bar \x) + \mathrm{d}^{r} (r_{i})_{\bar \x} \a \right\|^{2}.
\end{equation}
The optimal solution $\x^{*}$ of the left problem can be approximately retrieved from $\a^{*}$ as $\x^{*} = \bar \x \oplus_{r} \a^{*}$ assuming that $\a^{*}$ is small.

Letting $r_{i} \coloneq r_{i}(\bar \x)$ and $J_{i} \coloneq \mathrm{d}^{r} (r_{i})_{\bar \x}$ expanding the square and ignoring the constant term yields
\begin{equation}
  \min_{\a \in T_{\bar \x} \M} \sum_{i=1}^{N}\frac{1}{2} \a^{T} J_{i}^{T} J_{i} \a + \a^{T} J_{i}^{T} r_{i} = \min_{\a \in T_{\bar \x} \M} \frac{1}{2} \a^{T} \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right) \a + \a^{T} \sum_{i=1}^{N} J_{i}^{T} r_{i}.
\end{equation}
The optimal solution of this problem can be obtained by setting the gradient w.r.t. $\a$ to zero and is
\begin{equation}
  \a^{*} = -  \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} r_{i} \right).
\end{equation}
From this we can infer the sensitivity of $\a^{*}$ to noise in $r_{i}$: recalling that $\mathrm{Var}(A x + B y) = A \mathrm{Var}(x) A^{T} + B \mathrm{Var}(y) B^{T}$ we get that
\begin{equation}
  \a^{*} \sim \mathcal N \left( - \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} \bar r_{i} \right), \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} \Sigma_{i} J_{i} \right) \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \right).
\end{equation}
For the special case when all $r_{i}$'s have unit covariance, i.e. $\Sigma_{i} = I$, the expression simplifies to
\begin{equation}
  \a^{*} \sim \mathcal N \left( - \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} \bar r_{i} \right), \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \right).
\end{equation}
Given a residual $r(x) \sim \mathcal N(\bar r(x), \Sigma)$ a residual with unit covariance can be obtained by left-multiplying with the square root information matrix $\sqrt{I} \coloneq \Sigma^{-1/2}$:
\begin{equation}
  \label{eq:9}
  \sqrt{I} r(x) \sim \mathcal N\left(\sqrt{I} \bar r(x), I\right). \end{equation}
Scaling with $\sqrt{I}$ makes sense in many applications since it in effect scales the residual by the inverse noise magnitude.

\begin{important}
  For the unit covariance case $\Sigma_{i} = I$ the tangent space covariance of the optimal solution $x^{*}$ is
  \begin{equation}
    \left( \sum_{i=1}^{N} \left(\mathrm{d}^{r} (r_{i})_{x^{*}}\right)^{T} \mathrm{d}^{r} (r_{i})_{x^{*}} \right)^{\dagger}.
  \end{equation}
\end{important}


\section{Levenberg-Marquardt}

Resources
\begin{itemize}
  \item Original MINPACK manual: \url{https://www.netlib.org/minpack/}
\end{itemize}

LM Implementations
\begin{itemize}
  \item Original MINPACK in fortran
  \item cminpack (ported from fortran) \url{https://devernay.github.io/cminpack/}
  \item Eigen unsupported (ported from cminpack)
\end{itemize}


Consider a Lie group optimization problem
\begin{equation}
  \min_\x \frac{1}{2} \left\| f(\x) \right\|^2, \quad f : \M \rightarrow \mathbb{R}^m.
\end{equation}
We are interested in devising an iterative algorithm for minimizing this function.

Given a point $\x$ we can solve a local optimization problem to find step $\a \in T \M _\x$ that leads to an improved estimate $\x \oplus_r \a$. The optimization problem can be re-formulated in terms of $\a$ as
\begin{equation}
  \argmin_{\a} \left\| f(\x \oplus_r \a) \right\|^2.
\end{equation}
Since the problem is nonlinear we resort to linearization. To avoid stepping outside the region where the linearization is accurate we also limit the stepsize and obtain the new problem

The diagonal scaling matrix $D = \Diag (d_1, \ldots, d_n)$ is typically chosen so that a component $d_i$ is inversely proportional to the magnutide of the gradient in that direction, which has the effect of allowing larger steps in directions with low gradient. Common choices include $D = \sqrt{\Diag \left(\diag(J^T J) \right)}$ and $d_i = \left\| \left[ \mathrm{d}^r f_\x \right]_{\cdot, i} \right\|$---the norm of the $i$:th column of the jacobian.

A complete Levenberg-Marquardt procedure is shown in Algorithm \ref{algo:lm}. The crucial step occurs on line 3 and is discussed further below.
\begin{algorithm}
  \DontPrintSemicolon
  \KwData{Iteration variables: point $\x^k$, trust region $\Delta^k$, scaling parameters $d_i^k$ as diagonal matrix $D^{k}$}
  \KwResult{Updated iteration variables $\x^{k+1}$, $\Delta^{k+1}$, $d_i^{k+1}$}

  $r = f(\x)$\;
  $J = \mathrm{d}^r f_\x$\;
  $\a_\mathrm{LM} = \argmin_{\a : \| D^{k} \a \| \leq \Delta^{k} }\left\| r + J \a \right \|^2$ \tcp*[r]{calculate increment step}
  $\rho = \frac{ \| r \|^2 - \| f(\x \oplus_r \a_\mathrm{LM}) \|^2 }{\| r \|^2 - \| r + J \a_\mathrm{LM} \|^2}$ \tcp*[r]{actual to predicted reduction ratio}
  \uIf{$\rho \leq 0.25$}{
    $\Delta^{k+1} = \Delta^k / 2$ \tcp*[f]{decrease trust region}
  }
  \ElseIf(){$\rho \geq 0.75$}{
    $\Delta^{k+1} = 2 \Delta^{k}$ \tcp*[r]{increase trust region}
  }
  \uIf{$\rho \leq 0.0001$}{
    $\x^{k+1} = \x^k$ \tcp*[f]{reject step}
  }
  \Else {
    $\x^{k+1} = \x^k \oplus_r \a_\mathrm{LM}$  \tcp*[r]{accept step}
    $d_i^{k+1} = \max \left(d_i^k, \left\| \left[ \mathrm{d}^r f_{\x^{k+1}} \right]_{\cdot, i} \right\| \right)$ \tcp*[r]{update scaling parameters}
  }
  \caption{One iteration of the Levenberg-Marquardt algorithm.}
  \label{algo:lm}
\end{algorithm}

Calculation of the actual to predicted reduction ratio can be rewritten as
\begin{equation}
  \rho = \frac{1 - \left( \frac{ \| f(\x \oplus \a_\mathrm{LM}) \|}{\|r\|} \right)^2 }{\left( \frac{\| J \a \|}{\| r \|} \right)^2 + 2 \left( \frac{\sqrt{\lambda} \| D \a \|}{\| r \|} \right)^2}
\end{equation}
where we have used that $\| r \|^2 - \| r + J \a \|^2 = -2 \a^T J r - \a^T J^T J \a = \| J \a \|^2 + 2 \lambda \| D \a \|^2$ which is a consequence of \eqref{eq:lm_normaleqs}. This formulation has the benefit of avoiding subtraction of numbers of large magnitude which may cause floating point roundoff errors.


\subsection{Trust-Region Problem}

We discuss how to solve a trust-region problem on the form
\begin{equation}
  \label{eq:lm_minimize_trust}
  \argmin_{\a : \| D \a \| \leq \Delta}\left\|J \a + r \right \|^2,
\end{equation}
where $D$ is a diagonal scaling matrix and $\Delta$ is a maximal step size. This constrained problem can be transformed into an unconstrained problem.
\begin{theorem}
  \label{thm:lm}
  A vector $\a^*$ is a global minimizer of
  \begin{equation}
    \argmin_{\| D \a \| \leq \Delta} \frac{1}{2} \| J \a + r \|^2.
  \end{equation}
  if and only if there exists $\lambda \geq 0$ such that
  \begin{subequations}
    \begin{align}
      J^T J + \lambda D^T D \succeq 0,                              \\
      (J^T J + \lambda D^T D) \a = -J^T r,  \label{eq:lm_normaleqs} \\
      \lambda \left( \left\| D \a \right\| - \Delta \right)=0.
    \end{align}
  \end{subequations}
\end{theorem}
We provide an argument based on duality to support this fact, see e.g. \cite[Theorem 4.1]{nocedal_numerical_2006} for a more rigorous proof.
\begin{proof}
  Let the lagrangian of the problem be
  \begin{equation}
    L(\a, \lambda) = \frac{1}{2} \| J \a + r \|^2 + \frac{\lambda}{2} \left( \left\| D \a \right\|^2 - \Delta^2 \right),
  \end{equation}
  so that the optimization problem \eqref{eq:lm_minimize_trust} equivalently can be written $\inf_{\a} \sup_{\lambda \geq 0} L(\a, \lambda)$, since the value of the inner problem is $+\infty$ when the constraint $\| D \a \| \leq \Delta$ is not satisfied.

  Assuming that strong duality holds, the dual problem $\sup_{\lambda \geq 0} \inf_{\a} L(\a, \lambda)$ has the same optimal value. The inner infimum of the dual problem can be re-written as
  \begin{equation}
    \inf_{\a} L(\a, \lambda) = \frac{1}{2} \a^T ( J^T J + \lambda D^T D ) \a + r^T J \a - \lambda \frac{\Delta^2}{2}.
  \end{equation}
  This inner problem has value $-\infty$ unless $J^T J + \lambda D^T D \succeq 0$, so the outer supremum restricts $\lambda$ to values that imply positive semi-definiteness. In this case the finite optimal value is attained for $\a$ such that $\left(J^T J + \lambda D^T D \right) \a = -J^T r$ which reduces the dual problem to
  \begin{equation}
    \sup_{\lambda \geq 0} -\frac{1}{2} \a^T \left( J^T J + \lambda D^T D \right) \a - \lambda \frac{\Delta^2}{2}.
  \end{equation}
  Also this problem has a closed-form solution: either the optimal solution is attained at the boundary, i.e. $\lambda = 0$, or it is attained at zero derivative w.r.t. $\lambda$ which necessitates $\a^T D^T D \a + \Delta^2 = 0$. These two latter conditions imply that the complementarity condition $\lambda (\| D \a \| - \Delta) = 0$ holds.
\end{proof}

Equation \eqref{eq:lm_normaleqs} represents the normal equations for the least-squares problem
\begin{equation}
  \argmin_{\a} \frac{1}{2} \a^T \left( J^T J + \lambda D^T D \right) \a + r^T J \a.
\end{equation}
Equivalently, it can be written on the standard form
\begin{equation}
  \label{eq:lm_lsq}
  \argmin_{\a} \left\| \begin{bmatrix}
    J \\ \sqrt{\lambda} D
  \end{bmatrix} \a + \begin{bmatrix}
    r \\ 0
  \end{bmatrix} \right\|^2.
\end{equation}
For numerical stability it is preferable to solve a least-squares problem instead of directly solving the normal equations.

Theorem \ref{thm:lm} suggests that the trust-region problem can be recast as a least squares problem, but doing so requires knowledge of the parameter $\lambda$. In the following we show how the least squares problem can be efficiently solved assuming knowledge of $\lambda$, and then discuss an algorithm for finding $\lambda$.


\subsection{Solving the Least-Squares Problem}
\label{ssec:lm_lsq}

The least-squares problem
\begin{equation}
  \label{eq:lm_structured_ls}
  \argmin_{\a} \left\| \begin{bmatrix} J \\ \sqrt{\lambda} D^k \end{bmatrix} \a + \begin{bmatrix} r \\ 0 \end{bmatrix} \right\|^2
\end{equation}
has structure which can be exploited to find a solution. Consider a QR decomposition with column pivoting of $J$ s.t. $J P = Q R$, where $P \in \mathbb{R}^{n \times n}$ is a permutation matrix, $Q \in \mathbb{R}^{n \times n}$ is orthogonal, and $R \in \mathbb{R}^{n \times n}$ is upper-diagonal. If $\a$ is a minimizer of \eqref{eq:lm_structured_ls} it is also a minimizer of
\begin{equation}
  \argmin_{\a} \left\| \begin{bmatrix} Q^{T} J P \\ \sqrt{\lambda} P^{T} D^k P \end{bmatrix} P^{T} \a + \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2 =     \argmin_{\a} \left\| \begin{bmatrix} R \\ \sqrt{\lambda} P^{T} D^k P \end{bmatrix} P^{T} \a + \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2.
\end{equation}
Consider a second QR decomposition s.t.
\begin{equation}
  \label{eq:lm_qr}
  \begin{bmatrix}
    R \\ \sqrt{\lambda} P^{T} D^{k} P
  \end{bmatrix} = \tilde Q \begin{bmatrix} \tilde R \\ 0 \end{bmatrix}
\end{equation}
where $\tilde Q = \begin{bmatrix} \tilde Q_{11} & \tilde Q_{12} \\ \tilde Q_{21} & \tilde Q_{22} \end{bmatrix} \in \mathbb{R}^{2n \times 2n}$ is orthogonal and $\tilde R \in \mathbb{R}^{n \times n}$ is upper-diagonal and has rank $n$. Since there are only $n$ non-zero variables in the lower triangular part this step can be efficiently computed via $n (n+1) / 2$ Givens rotations. In these variables the least-squares problem takes the form
\begin{equation*}
  \argmin_{\a} \left\| \tilde Q \begin{bmatrix} \tilde R \\ 0 \end{bmatrix} P^{T} \a + \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2 =
  \argmin_{\a} \left\| \begin{bmatrix} \tilde R \\ 0 \end{bmatrix} P^{T} \a + \tilde Q^{T} \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2 = \argmin_{\a} \left\| \tilde R P^{T} \a + \tilde Q_{11}^{T} Q^{T} r \right\|^{2},
\end{equation*}
and it is now apparent that the optimal solution is
\begin{equation}
  \label{eq:5}
  \a_{\textrm{LM}} = - P \tilde R^{-1} \tilde Q_{11}^{T} Q^{T} r.
\end{equation}
When solving \eqref{eq:lm_structured_ls} repeatedly for different values of $\lambda$ only the second QR decomposition needs to be re-computed.

\subsection{Finding the LM Parameter}

To search for a parameter that satisfies the relations in Theorem \ref{thm:lm} consider the function
\begin{equation}
  \phi(\alpha) = \left\| D \left(J^{T} J + \alpha D^{T} D \right)^{-1} J^{T} r \right\| - \Delta.
\end{equation}
Note that $\phi(\alpha)$ can be evaluated by solving the a structured least-squares problem as discussed in Section \ref{ssec:lm_lsq}. With those variables we have
\begin{equation}
  \phi(\alpha) = \left\| D P \tilde R^{-1} \tilde Q_{11}^{T} Q^{T} r \right\| - \Delta
\end{equation}
where $\tilde R$ and $\tilde Q$ depend on $\alpha$ due to \eqref{eq:lm_qr}.

We are interested in finding a value of $\alpha > 0$ s.t. $\phi(\alpha) \approx 0$. If $\phi(0) \leq 0$ it must hold that $\lambda = 0$ in Theorem \ref{thm:lm}, so we disregard this case. We follow \cite{watson_levenberg-marquardt_1978} to construct an algorithm for the case $\phi(0) > 0$.

The function $\phi$ is strictly decreasing in $\alpha$, and approximately of the form $\phi(\alpha) \approx \tilde \phi(\alpha) = \frac{a}{b + \alpha} - \Delta$. Setting $\tilde \phi(\alpha)$ to zero then gives $\alpha = -b + a / \Delta$, and fitting $a$ and $b$ s.t. $\phi(\alpha_{k}) = \tilde \phi(\alpha_{k})$ and $\phi'(\alpha_{k}) = \tilde \phi'(\alpha_{k})$ gives the update rule
\begin{equation}
  \alpha_{k+1} = \alpha_{k} - \frac{\phi(\alpha_{k}) + \Delta}{\Delta} \frac{\phi(\alpha_{k})}{\phi'(\alpha_{k})}.
\end{equation}
To implement this algorithm not only $\phi$ needs to be calculated, but also its derivative.

\paragraph{Derivative of $\phi$} Introduce $q(\alpha) = D (J^{T} J + \alpha D^{T} D)^{-1} J^{T} r$ so that $\phi(\alpha) = \sqrt{q(\alpha)^{T} q(\alpha)} - \Delta$. The derivative of $q$ becomes
\begin{equation}
  q'(\alpha) = -D (J^{T} J + \alpha D^{T} D)^{-1} D^{T} D (J^{T} J + \alpha D^{T} D)^{-1} J^{T} r = -D (J^{T} J + \alpha D^{T} D) D^{T} q(\alpha).
\end{equation}
where we have utilized that the derivative of $(A + \alpha B)^{-1} (A + \alpha B)$ is zero which gives
\begin{equation}
  \frac{\mathrm{d}}{\mathrm{d}\alpha} (A + \alpha B)^{-1} = - (A + \alpha B)^{-1}) B (A + \alpha B)^{{-1}}.
\end{equation}
Therefore the derivative of $\phi$ becomes
\begin{equation}
  \phi'(\alpha) = \frac{q(\alpha)^{T} q'(\alpha)}{\| q(\alpha) \|} = -\frac{D^{T}q(\alpha))^{T} (J^{T} J + \alpha D^{T} D)^{-1} (D^{T} q(\alpha))}{\| q(\alpha) \|}.
\end{equation}

When utilizing the method in Section \ref{ssec:lm_lsq} the same QR decompositions $PJ = QR$ and \eqref{eq:lm_qr} can be leveraged to evaluate the expression efficiently.
\begin{equation}
  \begin{aligned}
    J^{T} J + \alpha D^{T} D = (QRP^{T})^{T} (QRP^{T}) + \alpha D^{T} D = P R^{T} R P^{T} + \alpha D^{T} D \\
    = P \begin{bmatrix} R^{T} & \sqrt{\alpha} (P^{T} D P)^{T} \end{bmatrix}\begin{bmatrix} R \\ \sqrt{\alpha} P^{T} D P \end{bmatrix} P^{T} \overset{\eqref{eq:lm_qr}}{=} P \tilde R^{T} \tilde R P^{T}.
  \end{aligned}
\end{equation}
Therefore
\begin{equation}
  \phi'(\alpha) = - \frac{\left\| \tilde R^{-T} P^{T} D^{T} q(\alpha) \right\|^{2}}{\| q(\alpha) \|} = - \| q(\alpha) \| \left\| \tilde R^{-T} \frac{P^{T} D^{T} q(\alpha) }{\| q(\alpha) \|} \right\|^{2}
\end{equation}
We conclude by writing down the algorithm from \cite{watson_levenberg-marquardt_1978} for finding $\lambda$.

\begin{algorithm}
  \DontPrintSemicolon
  \KwData{Matrices $J, D$, vector $r$, scalar $\Delta$}
  \KwResult{$\lambda$ s.t. $\phi(\lambda) \leq 0.1$}

  Calculate QR decomposition $J P = Q R$\;

  $l_{0} = \begin{cases} -\phi(0)/\phi'(0) & \text{if $J$ nonsingular} \\ 0 & \textrm{otherwise} \end{cases}$\;
  $u_{0} = \frac{\left\| (J D^{-1})^{T} r\right\|}{\Delta}$\;
  $\alpha_{0} = \sqrt{l_{0} u_{0}}$\;

  \Repeat{$|\phi| \leq 0.1 \Delta$} {
  If $\alpha_{k} \not \in [l_{k}, u_{k}]$, set $\alpha_{k} = \max\left(0.001 u_{k}, \sqrt{l_{k} u_{k}}\right)$\;

  Calculate QR decomposition $\begin{bmatrix} R \\ \sqrt{\lambda} P^{T} D^{k} P \end{bmatrix} = \tilde Q \begin{bmatrix} \tilde R \\ 0 \end{bmatrix}$\;

  $z = -P \tilde R^{-1} \tilde Q_{11}^{T} Q^{T} r$\;
  $\phi = \left\| D z \right\| - \Delta$\;
  $\phi' = - \| D z \| \left\| \tilde R^{-T} \frac{P^{T} D^{T} D z }{\| D z \|} \right\|^{2}$\;

  $l_{k+1} = \textrm{max}\left(l_{k}, \alpha_{k} - \frac{\phi}{\phi'}\right)$\;
  $u_{k+1} = \begin{cases} \alpha_{k} & \text{if $\phi < 0$} \\ u_{k} & \text{otherwise} \end{cases}$\;
  $\alpha_{k+1} = \alpha_{k} - \frac{\phi + \Delta}{\Delta} \frac{\phi}{\phi'}$\;
  }

  \caption{LM parameter algorithm}
  \label{algo:lm_parameter}
\end{algorithm}

\begin{remark}
  Ceres just uses $\lambda = 1 / \Delta$.

  Sparse strategy: as ceres use $\lambda = 1 / \Delta$ and use a sparse eigen solver for the step.
\end{remark}

