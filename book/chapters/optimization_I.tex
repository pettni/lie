% !TEX root = ../root.tex

\chapter{Nonlinear Least Squares}

Like how Lie groups thread the line between linear and nonlinear manifolds, the same can be said for the role of nonlinear least squares in optimization, which is a type of optimization problem is rich enough for to model a wide variety of situations, yet structured enough to be amenable to practical algorithms.

A non-linear least squares problem has the general form
\begin{equation}
  \label{eq:nlsq}
  \min_{\x \in \M} \frac{1}{2} \sum_{i=1}^{N} \left\| r_{i}(\x) \right\|^{2}, \quad r_{i} : \mathcal M \rightarrow \mathbb{R}^{n_{r_{i}}}.
\end{equation}
The manifold $\M$ can be a Lie group or a Lie group product $\x = (\x_{1}, \ldots, \x_{k}) \in \M_{1} \times \ldots \times \M_{k}$. For the latter case, typically not every residual depends on each member of the bundle, i.e. $r_{i}(\x) = r_{i}\left( \{ \x_{j}\}_{j \in I_{i}} \right)$ where $I_{i} \subset \{ 1, \ldots, k\}$ is a subset of variables.

\begin{remark}
  An equivalent problem with a single residual is $\min_{\x \in \M} \frac{1}{2} \| r(\x) \|^{2}$ for
  \begin{equation}
    r(\x) =
    \begin{bmatrix}
      r_{1}(\x) \\
      \vdots \\
      r_{k}(\x)
    \end{bmatrix}.
  \end{equation}
  Although the single residual formulation simplifies notation somewhat, in practice it is for large problems important to leverage the sparsity structure which is better exposed in \eqref{eq:nlsq}.
\end{remark}


\section{Solution Sensitivity}

In many applications the residuals $r_{i}(\x)$ are obtained from data and are therefore associated with uncertainty. In this situation it is natural to ask how sensitive the optimal solution of the nonlinear least squares problem is to noise in the data. Assume that the noise associated with each residual is Gaussian and independent of other residuals, i.e. that
\begin{equation}
  r_{i}(\x) \sim \mathcal N\left(\bar r_{i}(\x), I \right),
\end{equation}
and consider a point $\bar \x$. We expand the objective using a Taylor approximation as
\begin{equation}
  \min_{\bar \x \in \M} \frac{1}{2} \sum_{i=1}^{N} \left\| r_{i}(\bar \x) \right\|^{2} \approx \min_{\a \in T_{\bar \x} \M} \frac{1}{2}  \sum_{i=1}^{N} \left\| r_{i}(\bar \x) + \mathrm{d}^{r} (r_{i})_{\bar \x} \a \right\|^{2}.
\end{equation}
The optimal solution $\x^{*}$ of the left problem can be approximately retrieved from $\a^{*}$ as $\x^{*} = \bar \x \oplus_{r} \a^{*}$ assuming that $\a^{*}$ is small.

Letting $r_{i} \coloneq r_{i}(\bar \x)$ and $J_{i} \coloneq \mathrm{d}^{r} (r_{i})_{\bar \x}$ expanding the square and ignoring the constant term yields
\begin{equation}
  \min_{\a \in T_{\bar \x} \M} \sum_{i=1}^{N}\frac{1}{2} \a^{T} J_{i}^{T} J_{i} \a + \a^{T} J_{i}^{T} r_{i} = \min_{\a \in T_{\bar \x} \M} \frac{1}{2} \a^{T} \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right) \a + \a^{T} \sum_{i=1}^{N} J_{i}^{T} r_{i}.
\end{equation}
The optimal solution of this problem can be obtained by setting the gradient w.r.t. $\a$ to zero and is
\begin{equation}
  \a^{*} = -  \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} r_{i} \right).
\end{equation}
From this we can infer the sensitivity of $\a^{*}$ to noise in $r_{i}$: recalling that $\mathrm{Var}(A x + B y) = A \mathrm{Var}(x) A^{T} + B \mathrm{Var}(y) B^{T}$ we get that
\begin{equation}
  \a^{*} \sim \mathcal N \left( - \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} \bar r_{i} \right), \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} \Sigma_{i} J_{i} \right) \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \right).
\end{equation}
For the special case when all $r_{i}$'s have unit covariance, i.e. $\Sigma_{i} = I$, the expression simplifies to
\begin{equation}
  \a^{*} \sim \mathcal N \left( - \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \left( \sum_{i=1}^{N} J_{i}^{T} \bar r_{i} \right), \left( \sum_{i=1}^{N} J_{i}^{T} J_{i} \right)^{\dagger} \right).
\end{equation}
Given a residual $r(x) \sim \mathcal N(\bar r(x), \Sigma)$ a residual with unit covariance can be obtained by left-multiplying with the square root information matrix $\sqrt{I} \coloneq \Sigma^{-1/2}$:
\begin{equation}
  \label{eq:9}
  \sqrt{I} r(x) \sim \mathcal N\left(\sqrt{I} \bar r(x), I\right). \end{equation}
Scaling with $\sqrt{I}$ makes sense in many applications since it in effect scales the residual by the inverse noise magnitude.

\begin{important}
  For the unit covariance case $\Sigma_{i} = I$ the tangent space covariance of the optimal solution $x^{*}$ is
  \begin{equation}
    \left( \sum_{i=1}^{N} \left(\mathrm{d}^{r} (r_{i})_{x^{*}}\right)^{T} \mathrm{d}^{r} (r_{i})_{x^{*}} \right)^{\dagger}.
  \end{equation}
\end{important}


\section{Levenberg-Marquardt}

Resources
\begin{itemize}
  \item Original MINPACK manual: \url{https://www.netlib.org/minpack/}
\end{itemize}

LM Implementations
\begin{itemize}
  \item Original MINPACK in fortran
  \item cminpack (ported from fortran) \url{https://devernay.github.io/cminpack/}
  \item Eigen unsupported (ported from cminpack)
  \item scipy (calls cminpack)
  \item Ceres
  \item GTSAM
\end{itemize}


Consider a Lie group optimization problem
\begin{equation}
  \min_\x \frac{1}{2} \left\| f(\x) \right\|^2, \quad f : \M \rightarrow \mathbb{R}^m.
\end{equation}
We are interested in devising an iterative algorithm for minimizing this function.

Given a poimt $\x$ we can solve a local optimization problem to find step $\a \in T \M _\x$ that leads to an improved estimate $\x \oplus_r \a$. The optimization problem can be re-formulated in terms of $\a$ as
\begin{equation}
  \argmin_{\a} \left\| f(\x \oplus_r \a) \right\|^2.
\end{equation}
Since the problem is nonlinear we resort to linearization. To avoid stepping outside the region where the linearization is accurate we also limit the stepsize and obtain the new problem
\begin{equation}
  \label{eq:lm_minimize_trust}
  \argmin_{\a : \| D \a \| \leq \Delta}\left\| f(\x) + \mathrm{d}^r f_\x \a \right \|^2,
\end{equation}
where $D$ is a diagonal scaling matrix and $\Delta$ is a maximal step size.

To simplify notation let $J \coloneq \mathrm{d}^r f_\x$ and $r = f(\x)$ which simplifies the objective function to $\| J \a + r \|^2$. We next show that this constrained problem can be transformed into an unconstrained problem.
\begin{theorem}
  \label{thm:lm}
  A vector $\a^*$ is a global minimizer of
  \begin{equation}
    \argmin_{\| D \a \| \leq \Delta} \frac{1}{2} \| J \a + r \|^2.
  \end{equation}
  if and only if there exists $\lambda \geq 0$ such that
  \begin{subequations}
    \begin{align}
      J^T J + \lambda D^T D \succeq 0,                              \\
      (J^T J + \lambda D^T D) \a = -J^T r,  \label{eq:lm_normaleqs} \\
      \lambda \left( \left\| D \a \right\| - \Delta \right)=0.
    \end{align}
  \end{subequations}
\end{theorem}
We provide an argument based on duality to support this fact, see e.g. \cite[Theorem 4.1]{nocedal_numerical_2006} for a more rigorous proof.
\begin{proof}
  Let the lagrangian of the problem be
  \begin{equation}
    L(\a, \lambda) = \frac{1}{2} \| J \a + r \|^2 + \frac{\lambda}{2} \left( \left\| D \a \right\|^2 - \Delta^2 \right),
  \end{equation}
  so that the optimization problem \eqref{eq:lm_minimize_trust} equivalently can be written $\inf_{\a} \sup_{\lambda \geq 0} L(\a, \lambda)$, since the value of the inner problem is $+\infty$ when the constraint $\| D \a \| \leq \Delta$ is not satisfied.

  Assuming that strong duality holds, the dual problem $\sup_{\lambda \geq 0} \inf_{\a} L(\a, \lambda)$ has the same optimal value. The inner infimum of the dual problem can be re-written as
  \begin{equation}
    \inf_{\a} L(\a, \lambda) = \frac{1}{2} \a^T ( J^T J + \lambda D^T D ) \a + r^T J \a - \lambda \frac{\Delta^2}{2}.
  \end{equation}
  This inner problem has value $-\infty$ unless $J^T J + \lambda D^T D \succeq 0$, so the outer supremum restricts $\lambda$ to values that imply positive semi-definiteness. In this case the finite optimal value is attained for $\a$ such that $\left(J^T J + \lambda D^T D \right) \a = -J^T r$ which reduces the dual problem to
  \begin{equation}
    \sup_{\lambda \geq 0} -\frac{1}{2} \a^T \left( J^T J + \lambda D^T D \right) \a - \lambda \frac{\Delta^2}{2}.
  \end{equation}
  Also this problem has a closed-form solution: either the optimal solution is attained at the boundary, i.e. $\lambda = 0$, or it is attained at zero derivative w.r.t. $\lambda$ which necessitates $\a^T D^T D \a + \Delta^2 = 0$. These two latter conditions imply that the complementarity condition $\lambda (\| D \a \| - \Delta) = 0$ holds.
\end{proof}

Equation \eqref{eq:lm_normaleqs} represents the normal equations for the least-squares problem
\begin{equation}
  \argmin_{\a} \frac{1}{2} \a^T \left( J^T J + \lambda D^T D \right) \a + r^T J \a.
\end{equation}
Equivalently, it can be written on the standard form
\begin{equation}
  \label{eq:lm_lsq}
  \argmin_{\a} \left\| \begin{bmatrix}
    J \\ \sqrt{\lambda} D
  \end{bmatrix} \a + \begin{bmatrix}
    r \\ 0
  \end{bmatrix} \right\|^2.
\end{equation}
For numerical stability it is preferable to solve a least-squares problem instead of directly solving the normal equations.

The diagonal scaling matrix $D = \Diag (d_1, \ldots, d_n)$ is typically chosen so that a component $d_i$ is inversely proportional to the magnutide of the gradient in that direction, which has the effect of allowing larger steps in directions with low gradient. Common choices include $D = \sqrt{\Diag \left(\diag(J^T J) \right)}$ and $d_i = \left\| \left[ \mathrm{d}^r f_\x \right]_{\cdot, i} \right\|$---the norm of the $i$:th column of the jacobian.

A complete Levenberg-Marquardt procedure is shown in Algorithm \ref{algo:lm}. The parameter $\lambda$ that approximately satisfies the conditions in Theorem \ref{thm:lm} can be obtained algorithmically \cite{nocedal_numerical_2006,watson_levenberg-marquardt_1978}, with implementations available in e.g. MINPACK's \texttt{lmpar}\footnote{\url{https://www.netlib.org/minpack}}.

\begin{algorithm}
  \DontPrintSemicolon
  \KwData{Iteration variables: point $\x^k$, trust region $\Delta^k$, scaling parameters $d_i^k$ as diagonal matrix $D^{k}$}
  \KwResult{Updated iteration variables $\x^{k+1}$, $\Delta^{k+1}$, $d_i^{k+1}$}

  $r = f(\x)$\;
  $J = \mathrm{d}^r f_\x$\;
  $\lambda = \mathtt{lmpar}(J, r, D^k, \Delta^{k})$ \tcp*[r]{calculate LM parameter}
  $\a_\mathrm{LM} = \argmin_{\a} \left\| \begin{bmatrix} J \\ \sqrt{\lambda} D^k \end{bmatrix} \a + \begin{bmatrix} r \\ 0 \end{bmatrix} \right\|^2$ \tcp*[r]{solve for increment step}
  $\rho = \frac{ \| r \|^2 - \| f(\x \oplus_r \a_\mathrm{LM}) \|^2 }{\| r \|^2 - \| r + J \a_\mathrm{LM} \|^2}$ \tcp*[r]{actual to predicted reduction ratio}
  \uIf{$\rho \leq 0.25$}{
    $\Delta^{k+1} = \Delta^k / 2$ \tcp*[f]{decrease trust region}
  }
  \ElseIf(){$\rho \geq 0.75$}{
    $\Delta^{k+1} = 2 \Delta^{k}$ \tcp*[r]{increase trust region}
  }
  $d_i^{k+1} = \max \left(d_i^k, \left\| \left[ \mathrm{d}^r f_{\x^{k+1}} \right]_{\cdot, i} \right\| \right)$ \tcp*[r]{update scaling parameters}
  \lIf(\tcp*[f]{reject step}){$\rho \leq 0.0001$}{$\x^{k+1} = \x^k$}
  \lElse(\tcp*[f]{accept step}){ $\x^{k+1} = \x^k \oplus_r \a_\mathrm{LM}$}

  \caption{Single LM step}
  \label{algo:lm}
\end{algorithm}

The crucial parts of the algorithm are line 3 and 4; the best way to implement these operations depends on the size and sparsity structure of $J$.

Calculation of the actual to predicted reduction ratio can be rewritten as
\begin{equation}
  \rho = \frac{1 - \left( \frac{ \| f(\x \oplus \a_\mathrm{LM}) \|}{\|r\|} \right)^2 }{\left( \frac{\| J \a \|}{\| r \|} \right)^2 + 2 \left( \frac{\sqrt{\lambda} \| D \a \|}{\| r \|} \right)^2}
\end{equation}
where we have used that $\| r \|^2 - \| r + J \a \|^2 = -2 \a^T J r - \a^T J^T J \a = \| J \a \|^2 + 2 \lambda \| D \a \|^2$ which is a consequence of \eqref{eq:lm_normaleqs}. This formulation has the benefit of avoiding subtraction of numbers of large magnitude which may cause floating point roundoff errors.


\subsection{Solving the Least-Squares Problem}
The least-squares problem
\begin{equation}
  \label{eq:lm_structured_ls}
  \argmin_{\a} \left\| \begin{bmatrix} J \\ \sqrt{\lambda} D^k \end{bmatrix} \a + \begin{bmatrix} r \\ 0 \end{bmatrix} \right\|^2
\end{equation}
has structure which can be exploited to find a solution. Consider a QR decomposition with column pivoting of $J$ s.t. $J P = Q R$, where $P \in \mathbb{R}^{n \times n}$ is a permutation matrix, $Q \in \mathbb{R}^{n \times n}$ is orthogonal, and $R \in \mathbb{R}^{n \times n}$ is upper-diagonal. If $\a$ is a minimizer of \eqref{eq:lm_structured_ls} it is also a minimizer of
\begin{equation}
  \label{eq:lm_structured_ls}
  \argmin_{\a} \left\| \begin{bmatrix} Q^{T} J P \\ \sqrt{\lambda} P^{T} D^k P \end{bmatrix} P^{T} \a + \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2 =     \argmin_{\a} \left\| \begin{bmatrix} R \\ \sqrt{\lambda} P^{T} D^k P \end{bmatrix} P^{T} \a + \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2.
\end{equation}
Consider a second QR decomposition s.t.
\begin{equation}
  \begin{bmatrix}
    R \\ \sqrt{\lambda} P^{T} D^{k} P
  \end{bmatrix} = \tilde Q \begin{bmatrix} \tilde R \\ 0 \end{bmatrix}
\end{equation}
where $\tilde Q = \begin{bmatrix} \tilde Q_{11} & \tilde Q_{12} \\ \tilde Q_{21} & \tilde Q_{22} \end{bmatrix} \in \mathbb{R}^{2n \times 2n}$ is orthogonal and $\tilde R \in \mathbb{R}^{n \times n}$ is upper-diagonal and has rank $n$. In these variables the least-squares problem takes the form
\begin{equation*}
  \argmin_{\a} \left\| \tilde Q \begin{bmatrix} \tilde R \\ 0 \end{bmatrix} P^{T} \a + \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2 =
  \argmin_{\a} \left\| \begin{bmatrix} \tilde R \\ 0 \end{bmatrix} P^{T} \a + \tilde Q^{T} \begin{bmatrix} Q^{T} r \\ 0 \end{bmatrix} \right\|^2 = \argmin_{\a} \left\| \tilde R P^{T} \a + \tilde Q_{11}^{T} Q^{T} r \right\|^{2},
\end{equation*}
and it is now apparent that the optimal solution is
\begin{equation}
  \label{eq:5}
  \a_{\textrm{LM}} = - P \tilde R^{-1} \tilde Q_{11}^{T} Q^{T} r.
\end{equation}
When solving \eqref{eq:lm_structured_ls} repeatedly for different values of $\lambda$ only the second QR decomposition needs to be re-computed.

\subsection{Finding the LM Parameter}

\todo[inline]{Follow \cite{watson_levenberg-marquardt_1978}}

\begin{remark}
  Ceres just uses $\lambda = 1 / \Delta$.

  Dense strategy: mimic MINPACK with column QR decomposition

  Sparse strategy: as ceres use $\lambda = 1 / \Delta$ and use a sparse eigen solver for the step.
\end{remark}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../root"
%%% End:
